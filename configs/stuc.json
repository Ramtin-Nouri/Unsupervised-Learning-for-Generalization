{
    "gpus": [1],
    "epochs": 100,
    "batch_size": 4,
    "num_workers": 8,
    "use_joints": false,
    "early_stopping_patience": 10,
    "learning_rate": 0.001,
    "model":{
    	"lstm_hidden_size": 512,
   	    "lstm_num_layers": 1,
    	"convlstm_layers": [16,16,16],
        "dropout_classifier":0
    }
}
